{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project\n",
    "\n",
    "Kendra Chalkley\n",
    "\n",
    "CS 559 Machine Learning\n",
    "\n",
    "June 26, 2018\n",
    "\n",
    "## Motivation and Goals\n",
    "\n",
    "Goal: Write an agglomerative clustering algorithm for Reddit data\n",
    "\n",
    "\n",
    "### Collecting this Data\n",
    "\n",
    "I initially collected this data as part of a replication project following [In and Absolute State](http://journals.sagepub.com/doi/abs/10.1177/2167702617747074) (Al-Mosaiwi and Johnstone, 2018). The study explored the use of 'absolutist' words (always, never, completely, etc.) in anxiety- and depression-related forums and found that absolutist words were used at a significantly higher rate in these groups than in controls. The paper also compares the frequency of various other collections of words and dictionaries (pronouns, negative emotion words, et al.) available through the LIWC (Linguistic Inquiry and Word Count) software. \n",
    "\n",
    "I wanted to use distributed computing to investigate these frequencies in a larger body of work, specifically choosing Reddit data. Reddit.com is a website on which users can create and contribute to forums called 'subreddits.' Subreddits cover a vast number of subjects including various mental illnesses, computing and gaming, sports, politics, the economy, and a large number of 'adult' topics. The dataset I've collected to use here is in csv format and includes information about approximately 1200 subreddits to which users have contibuted text posts of at least 100 words in length. It includes the following columns: \n",
    "\n",
    "-Subreddit - The name a subreddit (<=20 characters)\n",
    "- Posts - A count of posts with 100 or more words in each subreddit\n",
    "- Wordcount - A sum of the total number of words in all posts in that subreddit\n",
    "- 65 additional columns, each containing the frequency of words in a specific dictionary \n",
    "\n",
    "  + 64 from an outdated version of LIWC plus the absolutist dictionary mentioned above.\n",
    "  + Freq - the frequency of words from each dictionary calculated as: \n",
    "  \n",
    "  $$\\frac{number\\ of\\ words\\ matching\\ dictionary}{number\\ of\\ total\\ words}$$\n",
    "  \n",
    "\n",
    "### Why agglomerative clustering?\n",
    "\n",
    "The original study, which I collected this data to replicate, selected their forums by google search. They analyzed a small collection of forums on a select number of topics. To approximate this approach, I tried running K-means clustering on this dataset for K between 5 and 25. No value of K clustering was obviously more effective than another, and, when projected onto the first two principal components, cluster boundaries seemed arbitrary and difficult to interpret. \n",
    "\n",
    "Hierarchical clustering is my next step in exploring this dataset, as I hope it will provide more insight into the relationship between clusters. \n",
    "\n",
    "## The Process\n",
    "\n",
    "\n",
    "### Packages and  Reading data\n",
    "\n",
    "For this project I am using numpy to store my purely numerical data execute operations over large arrays and matrixes. Providing additional support for numpy arrays is pandas, which is useful for accessing columns of data by a variable name and for labeling rows with an  index name. I am also using m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "inputs= pd.read_csv('medoutput.csv')\n",
    "subset_inputs= pd.read_csv('subset_medoutput.csv')\n",
    "\n",
    "freqs=inputs.loc[:,list(set(inputs.columns) - set(('subreddit','count(1)','sum(wordcount)')))]\n",
    "subset_freqs=subset_inputs.loc[:,list(set(inputs.columns) - set(('subreddit','count(1)','sum(wordcount)')))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocessing\n",
    "In preprocessing, the data was already in a .csv file from my previous project collecting these numbers for Machine Learning. The only change that was necessary was to normalize the values to between 0 and 1 so that naturally higher numbers, such as functional word frequency, didn't receive unnecessary priority over lower values, such as leisure words. I wrote my own functions to do that (even though it's a fairly simple procedure probably and probably available in a package) in order to %something about the value of practice and exercise or repetition%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    v2=vector\n",
    "    vmin=float(min(vector))\n",
    "    vmax=float(max(vector))\n",
    "    vrange=vmax-vmin\n",
    "\n",
    "#    print('vector', vector, vrange)\n",
    "    \n",
    "    l=len(vector)\n",
    "    for i in range(0,l):\n",
    "        dif=float(vector[i])-vmin\n",
    "#        print('dif',dif, vrange, dif/vrange)\n",
    "        v2[i]=dif/vrange\n",
    "#        print(vector[i])\n",
    "    return v2\n",
    "\n",
    "def normalizeall(df):\n",
    "    df2=df\n",
    "    i=df.shape[1]\n",
    "    for j in range(0,i):\n",
    "        df2[:,j]=normalize(df2[:,j])\n",
    "    return df2\n",
    "\n",
    "nfreqs=normalizeall(np.array(freqs))\n",
    "nsubset_freqs=normalizeall(np.array(subset_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "\n",
    "### Distance measures: Points and Clusters\n",
    "Need: Distance between points (Euclidean for now) outout= matrix, Distance between Clusters (average distance between points in each cluster)\n",
    "\n",
    "\n",
    "The algorithm requires several functions, one of which is a function to find the distance between two points of arbitrary dimensionality. My Function Point Distance calculates Euclidean distance, instead of L1, because of %these reasons which sound v important%.\n",
    "\n",
    "\n",
    "My next function is to find the center of the group of points. It simply takes the average of the values in all dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointDistance (point1, point2):\n",
    "    dimNum=len(point1)\n",
    "    if dimNum!=len(point2):\n",
    "        print(\"we have a dimensionality problem, Houston\")\n",
    "        print(dimNum,len(point2))       \n",
    "\n",
    "    dif=point1[:-1]-point2[:-1]\n",
    "    s=sum(dif.T*dif)\n",
    "    ssqrt=np.sqrt(s)\n",
    "    #print('qrt2',ssqrt)\n",
    "    return ssqrt\n",
    " \n",
    "def findCenter(points):\n",
    "    n=np.shape(points)[0]\n",
    "    d=np.shape(points)[1]\n",
    "    center=np.zeros(d)\n",
    "    for i in range(0,d):\n",
    "        center[i]= np.mean(points.iloc[:,i])\n",
    "    return center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrix\n",
    "\n",
    "Then the remaining steps are to calculate the distance between all groups and merge the closest groups. To calculate the distance between groups, I'm using the distance between the averages, instead of the average distance between points, for the sake of efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=nsubset_freqs.copy()\n",
    "n=np.shape(data)[0]\n",
    "d=np.shape(data)[1]\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "df['cluster']=df.index\n",
    "centers=df\n",
    "\n",
    "distanceMatrix=np.zeros((n,n))\n",
    "\n",
    "for i in range(0,n):\n",
    "    pointa = centers.iloc[i,]\n",
    "    for j in range(0,n):\n",
    "        if j<=i:\n",
    "            distanceMatrix[i][j]=None\n",
    "        else:\n",
    "            pointb = centers.iloc[j,] \n",
    "            distanceMatrix[i][j]=pointDistance(pointa, pointb)\n",
    "            #print(i,j,distanceMatrix[i][j])\n",
    "\n",
    "matrixCopy=distanceMatrix.copy()            \n",
    "history=pd.DataFrame({'round1': df['cluster']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Groups and Recording History\n",
    "\n",
    "To merge points, I'm simply changing the group label as represented by the number to the smaller of the two numbers. We have to change the label for all points in the larger group and then recalculate the center given the center for the smaller group with the new points. \n",
    "\n",
    "The process of finding where certain values equal a given number, using the packages that I'm using (specifically, numpy and pandas), is a bit arcane. I create a boolean mask array which has true values wherever the original array equals the values I'm searching for, and then performs certain actions in the original array indexed by the mask. I'm storing the current state of the cluster assignments in an array/matrix called History so I can see how the hierarchical information in the tree came together. I'm also tracking this in a two dimensional array that lists which groups have merged at each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kchalk/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: All-NaN slice encountered\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#distanceMatrix=matrixCopy.copy()\n",
    "#data=nsubset_freqs.copy()\n",
    "df=pd.DataFrame(data)\n",
    "df['cluster']=df.index\n",
    "centers=df.copy()\n",
    "merges=pd.DataFrame(np.zeros((n,3)),columns=['head', 'leaves', 'groupsize'])\n",
    "\n",
    "iternum=0\n",
    "while np.nanmin(distanceMatrix)>0:\n",
    "    minimum=np.nanmin(distanceMatrix)\n",
    "    \n",
    "# get indexes where distance is minimum. find smaller group\n",
    "    mask=np.isin(distanceMatrix, minimum)\n",
    "    a,b =np.where(mask)\n",
    "    groupa=min(a[0],b[0])\n",
    "    groupb=max(a[0],b[0])\n",
    "\n",
    "#    print('groupb', groupb, 'cluster', df.loc[groupb,'cluster'], 'groupa', groupa)\n",
    "    \n",
    "#Find indexes of points in old group.\n",
    "\n",
    "    mask=np.isin(df['cluster'], groupb )\n",
    "    index=np.where(mask)\n",
    "#    print(index)\n",
    "#    print('455',df.loc[455,'cluster'], '85', df.loc[85,'cluster',],'48',df.loc[48,'cluster'])\n",
    "    \n",
    "    df.loc[mask,'cluster']=int(df.loc[groupa,'cluster'])\n",
    "    \n",
    "    \n",
    "#    print('groupb', groupb, 'cluster', df.loc[groupb,'cluster'], 'groupa', groupa)\n",
    "\n",
    "    mask=np.isin(df['cluster'], groupa)\n",
    "    #print(mask)\n",
    "\n",
    "    groupPoints= df[mask]\n",
    "\n",
    "#    print(iternum, 'group', groupa,'n point(s) in group', len(groupPoints))\n",
    "    centers.iloc[groupa]=findCenter(groupPoints)\n",
    "    centers.iloc[groupb]=None\n",
    "\n",
    "    merges.loc[iternum,'head']=groupa\n",
    "    merges.loc[iternum,'leaves']=groupb\n",
    "    merges.loc[iternum,'groupsize']=len(groupPoints)\n",
    "\n",
    "#    print('new center',centers.iloc[groupa])\n",
    "\n",
    "    history[str(iternum)]=df['cluster']\n",
    "    distanceMatrix[groupb,:]=None\n",
    "    distanceMatrix[:,groupb]=None\n",
    "    \n",
    "          \n",
    "    i=groupa\n",
    "    pointa = centers.iloc[i]\n",
    "    for j in range(0,n):\n",
    "        if j>groupa:\n",
    "            pointb = centers.iloc[j,] \n",
    "            distanceMatrix[i][j]=pointDistance(pointa, pointb)\n",
    "            #print(\"changed a to b to value\", pointa, pointb)\n",
    "    iternum+=1\n",
    "    \n",
    "\n",
    "## recalculate distances between c and all other centers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sort_values() got an unexpected keyword argument 'desc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b6901fb9cbb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subreddit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmerges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'leaves'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sort_values() got an unexpected keyword argument 'desc'"
     ]
    }
   ],
   "source": [
    "history.index=subset_inputs['subreddit']\n",
    "\n",
    "merges.groupby('head').count().sort_values('leaves', desc=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs\n",
    "\n",
    "##### what I have graphed\n",
    "In this graph, I projected the data onto two dimensional PCA space. I am graphing all 600 points in a scatter plot colored by cluster. Because of the size of the dataset, the only thing that one can actually see is that the number of colors reduces as the algorithm runs longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA(n_components=11)\n",
    "pcs=pca.fit_transform(subset_freqs)\n",
    "plotData=pd.DataFrame(pcs, index=subset_inputs['subreddit'])\n",
    "\n",
    "plots=[]\n",
    "j=0\n",
    "\n",
    "fig = plt.figure(figsize=(14,28))\n",
    "\n",
    "\n",
    "for i in range(0,601, 50):\n",
    "    plotData['cluster']=history.loc[:,str(i)]\n",
    "    j+=1\n",
    "    fig.add_subplot(7, 2, j)\n",
    "    plt.scatter(x=plotData[0],y=plotData[1], c=plotData['cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### what I should graph\n",
    "I would like to have drawn trees and subtrees for showing the merge history of the clusters, but I experienced difficulty with graphing trees in python.\n",
    "\n",
    "Another graph that I could make is to show the points in a given group as time moves on, which would show at what point large clusters merge together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,28))\n",
    "\n",
    "j=0\n",
    "for i in range(0,299, 100):\n",
    "    plotData=pd.DataFrame(pcs, index=subset_inputs['subreddit'])\n",
    "    plotData['cluster']=history.loc[:,str(i)]\n",
    "    mask=np.isin(plotData['cluster'], [12,0,5,16,4,140,1,26,111])\n",
    "    plotData=plotData.loc[mask]\n",
    "    j+=1\n",
    "    fig.add_subplot(9, 3, j)\n",
    "    plt.scatter(x=plotData[0],y=plotData[1], c=plotData['cluster'], cmap='Paired')\n",
    "\n",
    "for i in range(300,639, 20):\n",
    "    plotData=pd.DataFrame(pcs, index=subset_inputs['subreddit'])\n",
    "    plotData['cluster']=history.loc[:,str(i)]\n",
    "    mask=np.isin(plotData['cluster'], [12,0,5,16,4,140,1,26,111])\n",
    "    plotData=plotData.loc[mask]\n",
    "    j+=1\n",
    "    fig.add_subplot(9, 3, j)\n",
    "    plt.scatter(x=plotData[0],y=plotData[1], c=plotData['cluster'], cmap='Paired')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Directions\n",
    "\n",
    "More data. More graphs. Point to point distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
